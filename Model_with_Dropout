import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch_geometric.datasets import Planetoid, Amazon
from torch_geometric.nn import GATConv, TransformerConv
import time
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
import psutil

# Load the dataset
#dataset = Planetoid(root='/tmp/Cora', name='Cora')
#dataset = Planetoid(root='/tmp/PubMed', name='PubMed')
dataset = Amazon(root='/tmp/Amazon', name='Computers') 
data = dataset[0]

# Move data to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
data = data.to(device)

epochs = 500

class DisentangleLayer(nn.Module):
    def __init__(self, input_dim, disentangled_dim, num_components):
        super(DisentangleLayer, self).__init__()
        self.fc = nn.Linear(input_dim, disentangled_dim * num_components)
        self.num_components = num_components
        self.disentangled_dim = disentangled_dim

    def forward(self, x):
        x = self.fc(x)
        return x.view(x.size(0), -1)

class GATStandard(nn.Module):
    def __init__(self, num_features, hidden_channels, out_channels, heads=1, dropout=0.6):
        super(GATStandard, self).__init__()
        self.conv1 = GATConv(num_features, hidden_channels, heads=heads, dropout=dropout)
        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout)
        self.dropout = nn.Dropout(p=0.7) 
        
    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = F.elu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)

class GATTransformer(nn.Module):
    def __init__(self, num_features, hidden_channels, out_channels, heads=1, dropout=0.6):
        super(GATTransformer, self).__init__()
        self.conv1 = GATConv(num_features, hidden_channels, heads=heads, dropout=dropout)
        self.transformer1 = TransformerConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout)  # Modified heads
        self.transformer2 = TransformerConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout)  # Added layer with modified heads
        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout)
        self.dropout = nn.Dropout(p=0.7)  # Ajusta el dropout a 0.6

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = F.relu(self.conv1(x, edge_index))
        x = self.dropout(x)  # Apply dropout
        x = F.relu(self.transformer1(x, edge_index))
        x = self.dropout(x)  # Apply dropout
        x = F.relu(self.transformer2(x, edge_index))  # Added layer
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)

class GATTransformerWithDisentanglement(GATTransformer):
    def __init__(self, num_features, hidden_channels, out_channels, disentangled_dim, num_components, heads=1, dropout=0.6):
        super().__init__(num_features, hidden_channels, out_channels, heads, dropout)
        self.transformer3 = TransformerConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout)  # Additional layer
        self.disentangle = DisentangleLayer(hidden_channels * heads, disentangled_dim, num_components)
        self.fc = nn.Linear(disentangled_dim * num_components, out_channels)
        self.dropout = nn.Dropout(p=0.7)  # Ajusta el dropout a 0.6

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = F.relu(self.conv1(x, edge_index))
        x = self.dropout(x)  # Apply dropout
        x = F.relu(self.transformer1(x, edge_index))
        x = self.dropout(x)  # Apply dropout
        x = F.relu(self.transformer2(x, edge_index))  # Added layer
        x = self.dropout(x)  # Apply dropout
        x = F.relu(self.transformer3(x, edge_index))  # Additional layer
        x = self.disentangle(x)
        x = self.fc(x)
        return F.log_softmax(x, dim=1)

def get_metrics(model, data, mask):
    model.eval()
    with torch.no_grad():
        logits = model(data)
        probs = F.softmax(logits[mask], dim=1).cpu().numpy()  # Use softmax to get probabilities
        preds = logits[mask].max(1)[1].cpu().numpy()
        labels = data.y[mask].cpu().numpy()
        
        accuracy = accuracy_score(labels, preds)
        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
        auc_roc = roc_auc_score(labels, probs, multi_class='ovr')  # Use probabilities instead of predicted classes
        
    return accuracy, precision, recall, f1, auc_roc

def train_and_evaluate(model, optimizer, data, epochs=700):
    training_losses = []
    validation_losses = []
    start_time = time.time()
    max_memory_usage = 0
    samples_processed = 0

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        out = model(data)
        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
        loss.backward()
        optimizer.step()

        model.eval()
        with torch.no_grad():
            out = model(data)
            val_loss = F.nll_loss(out[data.val_mask], data.y[data.val_mask])

        training_losses.append(loss.item())
        validation_losses.append(val_loss.item())
        
        # Measure memory usage
        max_memory_usage = max(max_memory_usage, psutil.virtual_memory().used)
        samples_processed += data.num_nodes

    end_time = time.time()
    training_time = end_time - start_time
    throughput = samples_processed / training_time

    # Metrics on validation set
    accuracy, precision, recall, f1, auc_roc = get_metrics(model, data, data.val_mask)

    return training_losses, validation_losses, training_time, max_memory_usage, throughput, accuracy, precision, recall, f1, auc_roc

def measure_inference_time(model, data, n_runs=100):
    model.eval()
    inference_times = []
    max_memory_usage = 0
    samples_processed = 0
    with torch.no_grad():
        for _ in range(n_runs):
            start_time = time.time()
            model(data)
            end_time = time.time()
            inference_times.append(end_time - start_time)
            
            # Measure memory usage
            max_memory_usage = max(max_memory_usage, psutil.virtual_memory().used)
            samples_processed += data.num_nodes
    
    avg_inference_time = sum(inference_times) / n_runs
    throughput = samples_processed / sum(inference_times)
    return avg_inference_time, max_memory_usage, throughput

def cross_validate(models, data, k=5):
    kf = KFold(n_splits=k, shuffle=True)
    results = {}

    for model_name, model_class in models.items():
        val_losses = []
        total_training_time = 0
        total_inference_time = 0
        total_training_memory = 0
        total_inference_memory = 0
        total_accuracy = 0
        total_precision = 0
        total_recall = 0
        total_f1 = 0
        total_auc_roc = 0
        total_training_throughput = 0
        total_inference_throughput = 0
        
        for train_index, val_index in kf.split(data.x):
            train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)
            val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)
            train_mask[train_index] = True
            val_mask[val_index] = True
            data.train_mask = train_mask
            data.val_mask = val_mask

            if model_name == 'GATTransformerWithDisentanglement':
                model = model_class(dataset.num_features, 8, dataset.num_classes, 4, 2, heads=8).to(device)
            else:
                model = model_class(dataset.num_features, 8, dataset.num_classes, heads=8).to(device)

            optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)
            _, val_loss, training_time, training_memory, training_throughput, accuracy, precision, recall, f1, auc_roc = train_and_evaluate(model, optimizer, data, epochs=epochs)
            val_losses.append(val_loss[-1])
            total_training_time += training_time
            total_training_memory += training_memory
            total_accuracy += accuracy
            total_precision += precision
            total_recall += recall
            total_f1 += f1
            total_auc_roc += auc_roc
            total_training_throughput += training_throughput

            # Measure inference time and memory usage
            inference_time, inference_memory, inference_throughput = measure_inference_time(model, data)
            total_inference_time += inference_time
            total_inference_memory += inference_memory
            total_inference_throughput += inference_throughput

        avg_val_loss = sum(val_losses) / len(val_losses)
        avg_training_time = total_training_time / k
        avg_inference_time = total_inference_time / k
        avg_training_memory = total_training_memory / k
        avg_inference_memory = total_inference_memory / k
        avg_accuracy = total_accuracy / k
        avg_precision = total_precision / k
        avg_recall = total_recall / k
        avg_f1 = total_f1 / k
        avg_auc_roc = total_auc_roc / k
        avg_training_throughput = total_training_throughput / k
        avg_inference_throughput = total_inference_throughput / k
        
        results[model_name] = {
            'Average Validation Loss': avg_val_loss,
            'Average Training Time': avg_training_time,
            'Average Inference Time': avg_inference_time,
            'Average Training Memory': avg_training_memory,
            'Average Inference Memory': avg_inference_memory,
            'Average Accuracy': avg_accuracy,
            'Average Precision': avg_precision,
            'Average Recall': avg_recall,
            'Average F1-Score': avg_f1,
            'Average AUC-ROC': avg_auc_roc,
            'Average Training Throughput': avg_training_throughput,
            'Average Inference Throughput': avg_inference_throughput
        }
        print(f'{model_name} - Average Validation Loss: {avg_val_loss}, Average Training Time: {avg_training_time}, Average Inference Time: {avg_inference_time}, Average Training Memory: {avg_training_memory}, Average Inference Memory: {avg_inference_memory}, Average Accuracy: {avg_accuracy}, Average Precision: {avg_precision}, Average Recall: {avg_recall}, Average F1-Score: {avg_f1}, Average AUC-ROC: {avg_auc_roc}, Average Training Throughput: {avg_training_throughput}, Average Inference Throughput: {avg_inference_throughput}')

    with open('metrics.txt', 'w') as f:
        for model_name, metrics in results.items():
            f.write(f'{model_name}\n')
            for metric_name, value in metrics.items():
                f.write(f'{metric_name}: {value}\n')
            f.write('\n')

    return results

# Initialize models
models = {
    'GATStandard': GATStandard,
    'GATTransformer': GATTransformer,
    'GATTransformerWithDisentanglement': GATTransformerWithDisentanglement
}

# Perform cross-validation
results = cross_validate(models, data, k=5)
